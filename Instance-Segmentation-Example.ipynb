{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation Using Habitat-Sim and Mask-R-CNN\n",
    "\n",
    "author: Michael Piseno (mpiseno@fb.com)\n",
    "\n",
    "This notebook will demonstrate how to set up an efficient datapipline for the purpose of instance segmentation using PyTorch, Mask-R-CNN, and Habitat-Sim as a data generator.\n",
    "\n",
    "Other resources:\n",
    "* [Mask-R-CNN paper](https://arxiv.org/pdf/1703.06870.pdf)\n",
    "* [PyTorch instance segmentation tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from habitat_sim.utils.data.dataextractor import ImageExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and proprocessing\n",
    "\n",
    "Below we will define the data extraction and preprocessing steps. Habitat-Sim's image data extraction API will be used to gather images from within the simulator for use inside a PyTorch Dataset subclass, which is subsequently fed into a PyTorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0204 15:12:17.414058 38362 simulator.py:131] Loaded navmesh ../../data/scene_datasets/habitat-test-scenes/skokloster-castle.navmesh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_cfg.physics_config_file = ./data/default.phys_scene_config.json\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "    \n",
    "class HabitatDataset(Dataset):\n",
    "    def __init__(self, extractor, transform=None):\n",
    "        self.extractor = extractor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.extractor.poses)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.extractor[idx]\n",
    "        img, mask = sample[\"rgb\"][:, :, :3], sample[\"semantic\"]\n",
    "        obj_ids = np.unique(mask)\n",
    "        masks = np.array([mask == obj_id for obj_id in obj_ids])\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "\n",
    "try:\n",
    "    extractor.close()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "scene_filepath = \"../../data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\" # Replace with your filepath\n",
    "extractor = ImageExtractor(scene_filepath, output=[\"rgb\", \"semantic\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate an ImageExtractor from Habitat-Sim. This requires that we previde a the filepath to a scene from which we will extract images. Optionally, we can specify the type of output we would like from the extractor. The default is just RGB images.\n",
    "\n",
    "```python\n",
    "scene_filepath = \"../../data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\"\n",
    "extractor = ImageExtractor(scene_filepath, output=[\"rgb\", \"semantic\"])\n",
    "```\n",
    "\n",
    "We then create a custom class that subclasses PyTorch's dataset and override the __len__ and __getitem__ methods. Mask-R-CNN requires that we provide the image, bounding boxes, semantic masks, and class labels for each example, so we have implemented functionality for that in the __getitem__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which transforms to apply to the data in preprocessing\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = HabitatDataset(extractor, transform=transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "num_classes = 10\n",
    "model_weights = \"maskrcnn-weights\"\n",
    "load_weights = True\n",
    "\n",
    "model = build_model(num_classes)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/3]  eta: 0:01:04  lr: 0.002502  loss: 4.6411 (4.6411)  loss_classifier: 2.3846 (2.3846)  loss_box_reg: 0.0750 (0.0750)  loss_mask: 2.0145 (2.0145)  loss_objectness: 0.0922 (0.0922)  loss_rpn_box_reg: 0.0749 (0.0749)  time: 21.5655  data: 0.2227\n",
      "Epoch: [0]  [2/3]  eta: 0:00:17  lr: 0.005000  loss: 4.9862 (6.0747)  loss_classifier: 2.3846 (1.9543)  loss_box_reg: 0.0750 (0.0649)  loss_mask: 2.0145 (2.0435)  loss_objectness: 0.0922 (1.4433)  loss_rpn_box_reg: 0.0749 (0.5687)  time: 17.2008  data: 0.0959\n",
      "Epoch: [0] Total time: 0:00:51 (17.2013 s / it)\n",
      "Epoch: [1]  [0/3]  eta: 0:00:58  lr: 0.005000  loss: 1.8169 (1.8169)  loss_classifier: 0.1961 (0.1961)  loss_box_reg: 0.0446 (0.0446)  loss_mask: 0.8425 (0.8425)  loss_objectness: 0.2259 (0.2259)  loss_rpn_box_reg: 0.5079 (0.5079)  time: 19.5119  data: 0.0408\n",
      "Epoch: [1]  [2/3]  eta: 0:00:16  lr: 0.005000  loss: 1.8169 (1.8073)  loss_classifier: 0.1961 (0.1966)  loss_box_reg: 0.0492 (0.0509)  loss_mask: 0.9531 (1.0600)  loss_objectness: 0.2380 (0.2927)  loss_rpn_box_reg: 0.0592 (0.2071)  time: 16.4183  data: 0.0357\n",
      "Epoch: [1] Total time: 0:00:49 (16.4188 s / it)\n",
      "Epoch: [2]  [0/3]  eta: 0:01:08  lr: 0.005000  loss: 1.3871 (1.3871)  loss_classifier: 0.1440 (0.1440)  loss_box_reg: 0.0600 (0.0600)  loss_mask: 0.7716 (0.7716)  loss_objectness: 0.0776 (0.0776)  loss_rpn_box_reg: 0.3340 (0.3340)  time: 22.6707  data: 0.0431\n",
      "Epoch: [2]  [2/3]  eta: 0:00:17  lr: 0.005000  loss: 1.1060 (1.1877)  loss_classifier: 0.1440 (0.1461)  loss_box_reg: 0.0600 (0.0627)  loss_mask: 0.7603 (0.7614)  loss_objectness: 0.0744 (0.0702)  loss_rpn_box_reg: 0.0550 (0.1473)  time: 17.8395  data: 0.0359\n",
      "Epoch: [2] Total time: 0:00:53 (17.8400 s / it)\n",
      "Epoch: [3]  [0/3]  eta: 0:00:59  lr: 0.000500  loss: 2.6873 (2.6873)  loss_classifier: 0.1356 (0.1356)  loss_box_reg: 0.0513 (0.0513)  loss_mask: 0.7126 (0.7126)  loss_objectness: 0.1076 (0.1076)  loss_rpn_box_reg: 1.6803 (1.6803)  time: 19.6725  data: 0.0481\n",
      "Epoch: [3]  [2/3]  eta: 0:00:17  lr: 0.000500  loss: 1.0175 (1.5639)  loss_classifier: 0.1354 (0.1277)  loss_box_reg: 0.0513 (0.0583)  loss_mask: 0.7171 (0.7176)  loss_objectness: 0.0537 (0.0638)  loss_rpn_box_reg: 0.0549 (0.5965)  time: 17.3951  data: 0.0374\n",
      "Epoch: [3] Total time: 0:00:52 (17.3956 s / it)\n",
      "Epoch: [4]  [0/3]  eta: 0:01:02  lr: 0.000500  loss: 1.8339 (1.8339)  loss_classifier: 0.1229 (0.1229)  loss_box_reg: 0.0596 (0.0596)  loss_mask: 0.7029 (0.7029)  loss_objectness: 0.6781 (0.6781)  loss_rpn_box_reg: 0.2704 (0.2704)  time: 20.8139  data: 0.0447\n",
      "Epoch: [4]  [2/3]  eta: 0:00:16  lr: 0.000500  loss: 0.9665 (1.2495)  loss_classifier: 0.0974 (0.1056)  loss_box_reg: 0.0569 (0.0568)  loss_mask: 0.7107 (0.7145)  loss_objectness: 0.0445 (0.2494)  loss_rpn_box_reg: 0.0571 (0.1232)  time: 16.7801  data: 0.0363\n",
      "Epoch: [4] Total time: 0:00:50 (16.7806 s / it)\n",
      "Epoch: [5]  [0/3]  eta: 0:00:58  lr: 0.000500  loss: 0.9488 (0.9488)  loss_classifier: 0.1017 (0.1017)  loss_box_reg: 0.0556 (0.0556)  loss_mask: 0.7110 (0.7110)  loss_objectness: 0.0384 (0.0384)  loss_rpn_box_reg: 0.0420 (0.0420)  time: 19.5847  data: 0.0431\n",
      "Epoch: [5]  [2/3]  eta: 0:00:16  lr: 0.000500  loss: 1.1133 (1.0925)  loss_classifier: 0.1060 (0.1418)  loss_box_reg: 0.0556 (0.0711)  loss_mask: 0.7007 (0.7021)  loss_objectness: 0.0429 (0.0635)  loss_rpn_box_reg: 0.0548 (0.1141)  time: 16.2628  data: 0.0352\n",
      "Epoch: [5] Total time: 0:00:48 (16.2632 s / it)\n",
      "Epoch: [6]  [0/3]  eta: 0:00:58  lr: 0.000050  loss: 1.0164 (1.0164)  loss_classifier: 0.1590 (0.1590)  loss_box_reg: 0.0785 (0.0785)  loss_mask: 0.6973 (0.6973)  loss_objectness: 0.0410 (0.0410)  loss_rpn_box_reg: 0.0406 (0.0406)  time: 19.6388  data: 0.0417\n",
      "Epoch: [6]  [2/3]  eta: 0:00:16  lr: 0.000050  loss: 1.0164 (1.1056)  loss_classifier: 0.1105 (0.1265)  loss_box_reg: 0.0568 (0.0589)  loss_mask: 0.6973 (0.6986)  loss_objectness: 0.0426 (0.0561)  loss_rpn_box_reg: 0.0549 (0.1654)  time: 16.3001  data: 0.0356\n",
      "Epoch: [6] Total time: 0:00:48 (16.3006 s / it)\n",
      "Epoch: [7]  [0/3]  eta: 0:00:58  lr: 0.000050  loss: 1.1983 (1.1983)  loss_classifier: 0.1561 (0.1561)  loss_box_reg: 0.0648 (0.0648)  loss_mask: 0.6954 (0.6954)  loss_objectness: 0.0569 (0.0569)  loss_rpn_box_reg: 0.2250 (0.2250)  time: 19.6658  data: 0.0431\n",
      "Epoch: [7]  [2/3]  eta: 0:00:16  lr: 0.000050  loss: 0.9678 (1.0365)  loss_classifier: 0.1164 (0.1293)  loss_box_reg: 0.0648 (0.0668)  loss_mask: 0.6954 (0.6965)  loss_objectness: 0.0479 (0.0416)  loss_rpn_box_reg: 0.0547 (0.1023)  time: 16.6012  data: 0.0363\n",
      "Epoch: [7] Total time: 0:00:49 (16.6016 s / it)\n",
      "Epoch: [8]  [0/3]  eta: 0:00:59  lr: 0.000050  loss: 1.1360 (1.1360)  loss_classifier: 0.1146 (0.1146)  loss_box_reg: 0.0593 (0.0593)  loss_mask: 0.7026 (0.7026)  loss_objectness: 0.0540 (0.0540)  loss_rpn_box_reg: 0.2055 (0.2055)  time: 19.8693  data: 0.0454\n",
      "Epoch: [8]  [2/3]  eta: 0:00:16  lr: 0.000050  loss: 1.0702 (1.0493)  loss_classifier: 0.1146 (0.1399)  loss_box_reg: 0.0593 (0.0664)  loss_mask: 0.6922 (0.6944)  loss_objectness: 0.0443 (0.0443)  loss_rpn_box_reg: 0.0546 (0.1043)  time: 16.4697  data: 0.0373\n",
      "Epoch: [8] Total time: 0:00:49 (16.4702 s / it)\n",
      "Epoch: [9]  [0/3]  eta: 0:00:59  lr: 0.000005  loss: 1.0239 (1.0239)  loss_classifier: 0.1674 (0.1674)  loss_box_reg: 0.0838 (0.0838)  loss_mask: 0.6923 (0.6923)  loss_objectness: 0.0407 (0.0407)  loss_rpn_box_reg: 0.0397 (0.0397)  time: 19.7313  data: 0.0445\n",
      "Epoch: [9]  [2/3]  eta: 0:00:16  lr: 0.000005  loss: 1.0239 (1.0420)  loss_classifier: 0.1180 (0.1331)  loss_box_reg: 0.0609 (0.0622)  loss_mask: 0.6923 (0.6955)  loss_objectness: 0.0447 (0.0484)  loss_rpn_box_reg: 0.0545 (0.1028)  time: 16.3200  data: 0.0361\n",
      "Epoch: [9] Total time: 0:00:48 (16.3205 s / it)\n"
     ]
    }
   ],
   "source": [
    "# Example of training\n",
    "from examples.instance_segmentation.engine import train_one_epoch\n",
    "import examples.instance_segmentation.utils\n",
    "\n",
    "\n",
    "if load_weights:\n",
    "    # Load model weights\n",
    "    pass\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        # Save model weights\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# # For Training\n",
    "# images, targets = next(iter(data_loader))\n",
    "# images = list(image for image in images)\n",
    "# targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "# output = model(images,targets)   # Returns losses and detections\n",
    "# # For inference\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
