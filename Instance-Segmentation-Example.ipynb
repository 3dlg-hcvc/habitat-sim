{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation Using Habitat-Sim and Mask-R-CNN\n",
    "\n",
    "author: Michael Piseno (mpiseno@gatech.edu)\n",
    "\n",
    "This notebook will demonstrate how to set up an efficient datapipline for the purpose of instance segmentation using PyTorch, Mask-R-CNN, and Habitat-Sim as a data generator.\n",
    "\n",
    "Other resources:\n",
    "* [Mask-R-CNN paper](https://arxiv.org/pdf/1703.06870.pdf)\n",
    "* [PyTorch instance segmentation tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from habitat_sim.utils.data.dataextractor import ImageExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and proprocessing\n",
    "\n",
    "Below we will define the data extraction and preprocessing steps. Habitat-Sim's image data extraction API will be used to gather images from within the simulator for use inside a PyTorch Dataset subclass, which is subsequently fed into a PyTorch dataloader. Also, we will define a function to filter our semantic mask output from the extractor to hide instances that we don't want. For example, if the semantic output has instances of wall, chair, table, pillow, and background, but we only want to do instance segmentation on chairs and tables, we will simply set the semantic mask pixel values for wall and pillow to be the same as background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extractor.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    " \n",
    "#scene = \"/private/home/mpiseno/Documents/Habitat/data/scene1/17DRP5sb8fy.glb\" # mp3d\n",
    "#scene = \"/private/home/mpiseno/Documents/Habitat/sorted_faces/18_scenes/apartment_1/mesh.ply\" # Replica\n",
    "scene_dir = \"/private/home/mpiseno/Documents/Habitat/data/\" # Replace with your scene directory\n",
    "extractor = ImageExtractor(scene, output=[\"rgba\", \"depth\", \"semantic\"],\n",
    "                           img_size=(1080, 1080), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the instance ids associated with the labels we specified and make a set from them\n",
    "from examples.instance_segmentation.common import create_mask_filter, area_filter\n",
    "\n",
    "# labels = ['bed', 'cushion', 'table', 'chair', 'sofa', 'tv_monitor',\n",
    "#           'floor', 'door', 'cabinet', 'counter', 'stool', 'blinds']\n",
    "\n",
    "labels = extractor.get_semantic_class_names()\n",
    "# Make sure background is class 0 so the mask_filter works properly\n",
    "labels = ['background'] + [name for name in labels if name not in ['background', 'void', '', 'objects']]\n",
    "\n",
    "mask_filter = create_mask_filter(labels, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))   \n",
    "    \n",
    "class HabitatDataset(Dataset):\n",
    "    def __init__(self, extractor, labels_we_care_about, transform=None):\n",
    "        self.extractor = extractor\n",
    "        self.transform = transform\n",
    "        self.instance_id_to_name = extractor.instance_id_to_name\n",
    "        if 0 not in self.instance_id_to_name:\n",
    "            self.instance_id_to_name[0] = 'background'\n",
    "        \n",
    "        # Create a mapping from class name to semantic ID\n",
    "        self.name_to_sem_id = {\n",
    "            name: id_val for id_val, name in enumerate(labels_we_care_about)\n",
    "        }\n",
    "        # And create the reverse mapping for convenience\n",
    "        self.sem_id_to_name = {\n",
    "            id_val: name for name, id_val in self.name_to_sem_id.items()\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.extractor)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.extractor[idx]\n",
    "        img, mask = sample[\"rgba\"][:, :, :3], sample[\"semantic\"]\n",
    "        mask = mask_filter(mask)\n",
    "        H, W = mask.shape\n",
    "        \n",
    "        instance_ids = np.unique(mask)\n",
    "        #instance_ids = instance_ids[1:] # We don't care about background\n",
    "        \n",
    "        # get bounding box coordinates, mask, and label for each instance_id\n",
    "        masks = []\n",
    "        labels = []\n",
    "        boxes = []\n",
    "        areas = []\n",
    "        num_instances = len(instance_ids)\n",
    "        \n",
    "        # There are much more efficient ways to create the data involving caching and\n",
    "        # preprocessing but efficiency is not the focus of this example\n",
    "        for i in range(num_instances):\n",
    "            cur_mask = mask == instance_ids[i]\n",
    "            pos = np.where(cur_mask)\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            \n",
    "            # Avoid zero area boxes\n",
    "            if xmin == xmax:\n",
    "                xmin = max(0, xmin - 1)\n",
    "                xmax = min(W, xmax + 1)\n",
    "            if ymin == ymax:\n",
    "                ymin = max(0, ymin - 1)\n",
    "                ymax = min(H, ymax + 1)\n",
    "            \n",
    "            box = (xmin, ymin, xmax, ymax)\n",
    "            if area_filter(cur_mask, box, H, W):\n",
    "                boxes.append(list(box))\n",
    "                masks.append(cur_mask)\n",
    "                name = self.instance_id_to_name[instance_ids[i]]\n",
    "                labels.append(self.name_to_sem_id[name])\n",
    "                areas.append((ymax - ymin) * (xmax - xmin))\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_instances,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate an ImageExtractor from Habitat-Sim. This requires that we previde either a the filepath to a scene or a filepath to a diretory which contains several scene files. Optionally, we can specify the type of output we would like from the extractor. The default is just RGBA images.For details on the ImageExtractor, refer to the \"Image-Data_Extraction-API\" notebook.\n",
    "\n",
    "Example\n",
    "```python\n",
    "scene_filepath = \"./data/scene1/skokloster-castle.glb\"\n",
    "extractor = ImageExtractor(scene_filepath, output=[\"rgb\", \"semantic\"])\n",
    "```\n",
    "\n",
    "We then create a custom class that subclasses PyTorch's dataset and override the __len__ and __getitem__ methods. Mask-R-CNN requires that we provide the image, bounding boxes, semantic masks, and class labels for each example, so we have implemented functionality for that in the __getitem__ method. The area and iscrowd keys are required for the evaluation metrics we use in this notebook.\n",
    "\n",
    "#### Setting up the datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which transforms to apply to the data in preprocessing\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = HabitatDataset(extractor, labels, transform=transform)\n",
    "dataset_test = HabitatDataset(extractor, labels, transform=transform)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, shuffle=False,\n",
    "                                          collate_fn=collate_fn)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=2, shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def show_data(data_idx, show_masks=False):\n",
    "    img, target = dataset_train[data_idx]\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    masks = target['masks'].numpy()\n",
    "    boxes = target['boxes'].numpy()\n",
    "    labels = target['labels']\n",
    "    areas = target['area']\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    cmap = {0: 'r', 1: 'g', 2: 'b', 3: 'c', 4: 'm', 5: 'y', 6: 'k', 7: 'w'}\n",
    "    for i, box in enumerate(boxes):\n",
    "        h, w = box[3] - box[1], box[2] - box[0]\n",
    "        rect = patches.Rectangle((box[0], box[1]) , w, h, linewidth=2,\n",
    "                                 edgecolor=cmap[i % len(cmap)], facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    \n",
    "    if show_masks:\n",
    "        plt.show()\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        columns = 4\n",
    "        rows = math.ceil(len(target['masks']) / columns)\n",
    "        for i in range(1, columns * rows + 1):\n",
    "            if i > len(target['masks']):\n",
    "                break\n",
    "            mask = masks[i - 1]\n",
    "            ax = fig.add_subplot(rows, columns, i)\n",
    "            sem_id = int(labels[i - 1].numpy())\n",
    "            ax.title.set_text(dataset_train.sem_id_to_name[sem_id])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(mask)\n",
    "        \n",
    "show_data(0, show_masks=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Credit: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "num_epochs = 160000\n",
    "num_classes = len(labels) + 1 # Number of labels we care about + background\n",
    "model_state_path = \"examples/instance_segmentation/runs/maskrcnn-example-state.pt\"\n",
    "load_state = False\n",
    "\n",
    "model = build_model(num_classes)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=4)\n",
    "writer = SummaryWriter(\"examples/instance_segmentation/runs/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.instance_segmentation.engine import train_one_epoch, load_model_state, save_model_state, evaluate\n",
    "\n",
    "epoch = 1\n",
    "if load_state:\n",
    "    params = {'lr': 0.002}\n",
    "    epoch = load_model_state(model, optimizer, model_state_path, params)\n",
    "    \n",
    "optimizer.lr = 0.002\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    # We have to explicitly set the extractor mode because there can only be one instance of an extractor at a time,\n",
    "    # so the dataset_train and dataset_test must share the same extractor\n",
    "    extractor.set_mode('train')\n",
    "    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=10, \n",
    "                    writer=writer, grad_clip=0, lr_scheduler=lr_scheduler)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        save_model_state(model, optimizer, epoch, model_state_path)\n",
    "        extractor.set_mode('test')\n",
    "        evaluate(model, dataloader_test, device=device)\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Testing ====\n",
    "from examples.instance_segmentation.engine import evaluate\n",
    "\n",
    "# Put the extractor into test mode so that it will use the test data\n",
    "extractor.set_mode('test')\n",
    "evaluator = evaluate(model, dataloader_test, device=device)\n",
    "print(dir(evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.instance_segmentation.common import InstanceVisualizer\n",
    "\n",
    "visualizer = InstanceVisualizer(model, dataloader_train, device=device, num_classes=num_classes)\n",
    "visuals = visualizer.visualize_instance_segmentation_output(max_num_outputs=12)\n",
    "# Visualize the output\n",
    "for idx in visuals.keys():\n",
    "    vis = visuals[idx]\n",
    "    #values = np.unique(vis.ravel())\n",
    "    im = plt.imshow(vis)\n",
    "    # colors = [im.cmap(im.norm(value)) for value in values]\n",
    "    # patches = [patches.Patch(color=colors[i], label=dataset_train.sem_id_to_name[value]) for i, value in enumerate(values)]\n",
    "    # # put those patched as legend-handles into the legend\n",
    "    # plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in dataloader_train:\n",
    "    img1, img2 = img\n",
    "    img1 = img1.permute(1, 2, 0).numpy()\n",
    "    img2 = img2.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(img1)\n",
    "    plt.show()\n",
    "    plt.imshow(img2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
