{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from habitat_sim.utils.data.dataextractor import ImageExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "IMG_HEIGHT = IMG_WIDTH = 32\n",
    "\n",
    "# Credit: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, extractor, root, transform=None):\n",
    "        self.extractor = extractor\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"sample_images\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"sample_masks\"))))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"sample_images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"sample_masks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        mask = mask[:, :, 0]\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        masks = np.array([mask == obj_id for obj_id in obj_ids])\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    \n",
    "class HabitatDataset(Dataset):\n",
    "    def __init__(self, extractor):\n",
    "        self.extractor = extractor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.extractor.poses)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.extractor[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0204 11:21:03.826827 25504 simulator.py:131] Loaded navmesh ../../data/scene_datasets/habitat-test-scenes/skokloster-castle.navmesh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_cfg.physics_config_file = ./data/default.phys_scene_config.json\n"
     ]
    }
   ],
   "source": [
    "# Setup training environment\n",
    "N_EPOCHS = 10\n",
    "\n",
    "model = get_model_instance_segmentation(NUM_CLASSES)\n",
    "\n",
    "# transform = T.Compose([T.ToTensor()])\n",
    "# dataset = MyDataset(None, root=\"./data/sample_data/\", transform=transform)\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "#                                           collate_fn=collate_fn)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "scene_filepath = \"../../data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\"\n",
    "extractor = ImageExtractor(scene_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = HabitatDataset(extractor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/1]  eta: 0:00:32  lr: 0.005000  loss: 5.5464 (5.5464)  loss_classifier: 2.3515 (2.3515)  loss_box_reg: 0.1789 (0.1789)  loss_mask: 2.0460 (2.0460)  loss_objectness: 0.7072 (0.7072)  loss_rpn_box_reg: 0.2628 (0.2628)  time: 32.9171  data: 0.3392\n",
      "Epoch: [0] Total time: 0:00:32 (32.9636 s / it)\n",
      "Epoch: [1]  [0/1]  eta: 0:00:34  lr: 0.005000  loss: 2.8464 (2.8464)  loss_classifier: 1.3775 (1.3775)  loss_box_reg: 0.2144 (0.2144)  loss_mask: 0.8798 (0.8798)  loss_objectness: 0.1465 (0.1465)  loss_rpn_box_reg: 0.2282 (0.2282)  time: 34.6248  data: 0.4232\n",
      "Epoch: [1] Total time: 0:00:34 (34.8419 s / it)\n",
      "Epoch: [2]  [0/1]  eta: 0:00:35  lr: 0.005000  loss: 2.0976 (2.0976)  loss_classifier: 0.5855 (0.5855)  loss_box_reg: 0.2643 (0.2643)  loss_mask: 0.9889 (0.9889)  loss_objectness: 0.0590 (0.0590)  loss_rpn_box_reg: 0.1998 (0.1998)  time: 35.3224  data: 0.4360\n",
      "Epoch: [2] Total time: 0:00:35 (35.5724 s / it)\n",
      "Epoch: [3]  [0/1]  eta: 0:00:35  lr: 0.000500  loss: 1.8664 (1.8664)  loss_classifier: 0.4779 (0.4779)  loss_box_reg: 0.2652 (0.2652)  loss_mask: 0.9080 (0.9080)  loss_objectness: 0.0458 (0.0458)  loss_rpn_box_reg: 0.1695 (0.1695)  time: 35.1919  data: 0.4347\n",
      "Epoch: [3] Total time: 0:00:35 (35.4253 s / it)\n",
      "Epoch: [4]  [0/1]  eta: 0:00:35  lr: 0.000500  loss: 1.8332 (1.8332)  loss_classifier: 0.4666 (0.4666)  loss_box_reg: 0.2537 (0.2537)  loss_mask: 0.8807 (0.8807)  loss_objectness: 0.0652 (0.0652)  loss_rpn_box_reg: 0.1669 (0.1669)  time: 35.3935  data: 0.4217\n",
      "Epoch: [4] Total time: 0:00:35 (35.6280 s / it)\n",
      "Epoch: [5]  [0/1]  eta: 0:00:35  lr: 0.000500  loss: 1.7823 (1.7823)  loss_classifier: 0.4777 (0.4777)  loss_box_reg: 0.2601 (0.2601)  loss_mask: 0.8438 (0.8438)  loss_objectness: 0.0374 (0.0374)  loss_rpn_box_reg: 0.1633 (0.1633)  time: 35.4881  data: 0.4248\n",
      "Epoch: [5] Total time: 0:00:35 (35.7268 s / it)\n",
      "Epoch: [6]  [0/1]  eta: 0:00:35  lr: 0.000050  loss: 1.7698 (1.7698)  loss_classifier: 0.4778 (0.4778)  loss_box_reg: 0.2595 (0.2595)  loss_mask: 0.8008 (0.8008)  loss_objectness: 0.0730 (0.0730)  loss_rpn_box_reg: 0.1588 (0.1588)  time: 35.6979  data: 0.4312\n",
      "Epoch: [6] Total time: 0:00:35 (35.9371 s / it)\n",
      "Epoch: [7]  [0/1]  eta: 0:00:35  lr: 0.000050  loss: 1.7429 (1.7429)  loss_classifier: 0.4722 (0.4722)  loss_box_reg: 0.2560 (0.2560)  loss_mask: 0.7955 (0.7955)  loss_objectness: 0.0610 (0.0610)  loss_rpn_box_reg: 0.1583 (0.1583)  time: 35.2214  data: 0.4136\n",
      "Epoch: [7] Total time: 0:00:35 (35.4564 s / it)\n",
      "Epoch: [8]  [0/1]  eta: 0:00:35  lr: 0.000050  loss: 1.7258 (1.7258)  loss_classifier: 0.4762 (0.4762)  loss_box_reg: 0.2611 (0.2611)  loss_mask: 0.7883 (0.7883)  loss_objectness: 0.0424 (0.0424)  loss_rpn_box_reg: 0.1577 (0.1577)  time: 35.4884  data: 0.4283\n",
      "Epoch: [8] Total time: 0:00:35 (35.7300 s / it)\n",
      "Epoch: [9]  [0/1]  eta: 0:00:35  lr: 0.000005  loss: 1.7290 (1.7290)  loss_classifier: 0.4783 (0.4783)  loss_box_reg: 0.2605 (0.2605)  loss_mask: 0.7832 (0.7832)  loss_objectness: 0.0499 (0.0499)  loss_rpn_box_reg: 0.1571 (0.1571)  time: 35.3445  data: 0.4191\n",
      "Epoch: [9] Total time: 0:00:35 (35.5801 s / it)\n"
     ]
    }
   ],
   "source": [
    "# Example of training\n",
    "from examples.instance_segmentation.engine import train_one_epoch\n",
    "import examples.instance_segmentation.utils\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "# # For Training\n",
    "# images, targets = next(iter(data_loader))\n",
    "# images = list(image for image in images)\n",
    "# targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "# output = model(images,targets)   # Returns losses and detections\n",
    "# # For inference\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
