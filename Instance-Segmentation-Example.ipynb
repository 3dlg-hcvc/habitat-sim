{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation Using Habitat-Sim and Mask-R-CNN\n",
    "\n",
    "author: Michael Piseno (mpiseno@fb.com)\n",
    "\n",
    "This notebook will demonstrate how to set up an efficient datapipline for the purpose of instance segmentation using PyTorch, Mask-R-CNN, and Habitat-Sim as a data generator.\n",
    "\n",
    "Other resources:\n",
    "* [Mask-R-CNN paper](https://arxiv.org/pdf/1703.06870.pdf)\n",
    "* [PyTorch instance segmentation tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from habitat_sim.utils.data.dataextractor import ImageExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and proprocessing\n",
    "\n",
    "Below we will define the data extraction and preprocessing steps. Habitat-Sim's image data extraction API will be used to gather images from within the simulator for use inside a PyTorch Dataset subclass, which is subsequently fed into a PyTorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0205 18:06:46.267751 69883 simulator.py:131] Loaded navmesh /datasets01/mp3d/073118/v1/habitat/17DRP5sb8fy/17DRP5sb8fy.navmesh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_cfg.physics_config_file = ./data/default.phys_scene_config.json\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "    \n",
    "class HabitatDataset(Dataset):\n",
    "    def __init__(self, extractor, transform=None):\n",
    "        self.extractor = extractor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.extractor)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.extractor[idx]\n",
    "        img, mask = sample[\"rgb\"][:, :, :3], sample[\"semantic\"]\n",
    "        H, W = mask.shape\n",
    "        \n",
    "        obj_ids = np.unique(mask)\n",
    "        masks = np.array([mask == obj_id for obj_id in obj_ids])\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            \n",
    "            # Avoid zero area boxes\n",
    "            if xmin == xmax:\n",
    "                xmin = max(0, xmin - 1)\n",
    "                xmax = min(W, xmax + 1)\n",
    "            if ymin == ymax:\n",
    "                ymin = max(0, ymin - 1)\n",
    "                ymax = min(H, ymax + 1)\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "\n",
    "try:\n",
    "    extractor.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "  \n",
    "scene_filepath = \"/datasets01/mp3d/073118/v1/habitat/17DRP5sb8fy/17DRP5sb8fy.glb\"\n",
    "#scene_filepath = \"../../data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\" # Replace with your filepath\n",
    "extractor = ImageExtractor(scene_filepath, output=[\"rgb\", \"semantic\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate an ImageExtractor from Habitat-Sim. This requires that we previde a the filepath to a scene from which we will extract images. Optionally, we can specify the type of output we would like from the extractor. The default is just RGB images.\n",
    "\n",
    "```python\n",
    "scene_filepath = \"../../data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\"\n",
    "extractor = ImageExtractor(scene_filepath, output=[\"rgb\", \"semantic\"])\n",
    "```\n",
    "\n",
    "We then create a custom class that subclasses PyTorch's dataset and override the __len__ and __getitem__ methods. Mask-R-CNN requires that we provide the image, bounding boxes, semantic masks, and class labels for each example, so we have implemented functionality for that in the __getitem__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "extractor.pose_extractor._show_topdown_view(show_valid_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which transforms to apply to the data in preprocessing\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = HabitatDataset(extractor, transform=transform)\n",
    "dataset_test = HabitatDataset(extractor, transform=transform)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, shuffle=False,\n",
    "                                          collate_fn=collate_fn)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=2, shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANfklEQVR4nO3cfcyddX3H8fdnfUJFqTys6dpmxdjF8IdD0iAGsziIG3bG8gcajJmNadJkc4nGJa5syRaT/aH7Q9Rk0TWDrC4+wHwIDWFjWDDL/hCpUpCHITdEQivaiIAuRAb63R/nV3bor3if9j7Xfc6dvV/Jyfk9nfv6nvbup9d1nes6qSokadxvzLoASfPHYJDUMRgkdQwGSR2DQVLHYJDUGSQYklyR5KEkC0n2DrENScPJtK9jSLIK+D7wduAIcBfw3qp6YKobkjSYIfYYLgYWqurRqvof4MvAzgG2I2kgqwf4mZuAx8f6R4A3/7oXrM26OoNXDVCKpON+zlM/qarzJlk7RDBMJMkeYA/AGbySN+fyWZUi/b/wjfrKY5OuHeJQ4iiwZay/uY29RFXtq6rtVbV9DesGKEPS6RoiGO4CtiU5P8la4GrgwADbkTSQqR9KVNULSf4MuBVYBVxfVfdPezuShjPIOYaqugW4ZYifLWl4XvkoqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOoYDJI6BoOkjsEgqWMwSOosGgxJrk9yLMl9Y2NnJ7ktycPt+bVtPEk+k2Qhyb1JLhqyeEnDmGSP4Z+AK04Y2wscrKptwMHWB3gHsK099gCfnU6ZkpbTosFQVf8B/PSE4Z3A/tbeD1w5Nv75GvkWsD7JxmkVK2l5nO45hg1V9URr/wjY0NqbgMfH1h1pY50ke5IcSnLoeZ47zTIkDWHJJx+rqoA6jdftq6rtVbV9DeuWWoakKTrdYPjx8UOE9nysjR8Ftoyt29zGJK0gpxsMB4Bdrb0LuGls/P3t04lLgGfGDjkkrRCrF1uQ5EvA24BzkxwB/gb4OHBjkt3AY8B72vJbgB3AAvAs8IEBapY0sEWDoare+zJTl59kbQEfXGpRkmbLKx8ldQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdQwGSR2DQVLHYJDUMRgkdRYNhiRbktyR5IEk9yf5UBs/O8ltSR5uz69t40nymSQLSe5NctHQb0LSdE2yx/AC8OdVdQFwCfDBJBcAe4GDVbUNONj6AO8AtrXHHuCzU69a0qAWDYaqeqKqvtvaPwceBDYBO4H9bdl+4MrW3gl8vka+BaxPsnHqlUsazCmdY0iyFXgTcCewoaqeaFM/Aja09ibg8bGXHWljklaIiYMhyZnAV4EPV9XPxueqqoA6lQ0n2ZPkUJJDz/PcqbxU0sAmCoYkaxiFwheq6mtt+MfHDxHa87E2fhTYMvbyzW3sJapqX1Vtr6rta1h3uvVLGsAkn0oEuA54sKo+OTZ1ANjV2ruAm8bG398+nbgEeGbskEPSCrB6gjWXAn8MfC/J4Tb2l8DHgRuT7AYeA97T5m4BdgALwLPAB6ZasaTBLRoMVfWfQF5m+vKTrC/gg0usS9IMeeWjpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKmzaDAkOSPJt5Pck+T+JB9r4+cnuTPJQpIbkqxt4+taf6HNbx32LUiatkn2GJ4DLquq3wUuBK5IcgnwCeDaqno98BSwu63fDTzVxq9t6yStIIsGQ438d+uuaY8CLgO+0sb3A1e29s7Wp81fniRTq1jS4CY6x5BkVZLDwDHgNuAR4OmqeqEtOQJsau1NwOMAbf4Z4JyT/Mw9SQ4lOfQ8zy3tXUiaqomCoap+WVUXApuBi4E3LHXDVbWvqrZX1fY1rFvqj5M0Raf0qURVPQ3cAbwFWJ9kdZvaDBxt7aPAFoA2fxbw5FSqlbQsJvlU4rwk61v7FcDbgQcZBcRVbdku4KbWPtD6tPnbq6qmWbSkYa1efAkbgf1JVjEKkhur6uYkDwBfTvK3wN3AdW39dcA/J1kAfgpcPUDdkga0aDBU1b3Am04y/iij8w0njv8CePdUqpM0E175KKljMEjqGAySOnMRDL/zxmdnXYKkMXMRDAC3/vAwt/7w8KzLkMQcBYOk+THJdQzL4g9/68JZlyCpmYs9hu/f+8pZlyBpzFwEg6T5YjBI6hgMkjoGg6SOwSCpYzBI6hgMkjoGg6SOwSCpYzBI6hgMkjoGg6SOwSCpYzBI6hgMkjoGg6SOwSCpYzBI6hgMkjoGg6SOwSCpYzBI6hgMkjoGg6SOwSCpM3EwJFmV5O4kN7f++UnuTLKQ5IYka9v4utZfaPNbhyld0lBOZY/hQ8CDY/1PANdW1euBp4DdbXw38FQbv7atk7SCTBQMSTYDfwT8Y+sHuAz4SluyH7iytXe2Pm3+8rZe0gox6R7Dp4CPAr9q/XOAp6vqhdY/Amxq7U3A4wBt/pm2/iWS7ElyKMmh53nuNMuXNIRFgyHJO4FjVfWdaW64qvZV1faq2r6GddP80ZKWaPUEay4F3pVkB3AG8Brg08D6JKvbXsFm4GhbfxTYAhxJsho4C3hy6pVLGsyiewxVdU1Vba6qrcDVwO1V9T7gDuCqtmwXcFNrH2h92vztVVVTrVrSoJZyHcNfAB9JssDoHMJ1bfw64Jw2/hFg79JKlLTcJjmUeFFVfRP4Zms/Clx8kjW/AN49hdokzYhXPkrqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjqGAySOhMFQ5IfJPleksNJDrWxs5PcluTh9vzaNp4kn0mykOTeJBcN+QYkTd+p7DH8flVdWFXbW38vcLCqtgEHWx/gHcC29tgDfHZaxUpaHks5lNgJ7G/t/cCVY+Ofr5FvAeuTbFzCdiQts0mDoYB/T/KdJHva2IaqeqK1fwRsaO1NwONjrz3Sxl4iyZ4kh5Icep7nTqN0SUNZPeG6t1bV0SS/CdyW5L/GJ6uqktSpbLiq9gH7AF6Ts0/ptZKGNdEeQ1Udbc/HgK8DFwM/Pn6I0J6PteVHgS1jL9/cxiStEIsGQ5JXJXn18TbwB8B9wAFgV1u2C7iptQ8A72+fTlwCPDN2yCFpBZjkUGID8PUkx9d/sar+LcldwI1JdgOPAe9p628BdgALwLPAB6ZetaRBpWr2h/dJfg48NOs6JnQu8JNZFzGBlVInrJxaV0qdcPJaf7uqzpvkxZOefBzaQ2PXR8y1JIdWQq0rpU5YObWulDph6bV6SbSkjsEgqTMvwbBv1gWcgpVS60qpE1ZOrSulTlhirXNx8lHSfJmXPQZJc2TmwZDkiiQPtdu09y7+ikFruT7JsST3jY3N5e3lSbYkuSPJA0nuT/Kheaw3yRlJvp3knlbnx9r4+UnubPXckGRtG1/X+gttfuty1DlW76okdye5ec7rHParEKpqZg9gFfAI8DpgLXAPcMEM6/k94CLgvrGxvwP2tvZe4BOtvQP4VyDAJcCdy1zrRuCi1n418H3ggnmrt23vzNZeA9zZtn8jcHUb/xzwJ639p8DnWvtq4IZl/nP9CPBF4ObWn9c6fwCce8LY1P7ul+2NvMybewtw61j/GuCaGde09YRgeAjY2NobGV1zAfAPwHtPtm5Gdd8EvH2e6wVeCXwXeDOji29Wn/h7ANwKvKW1V7d1Wab6NjP6bpHLgJvbP6S5q7Nt82TBMLW/+1kfSkx0i/aMLen28uXQdmPfxOh/47mrt+2eH2Z0o91tjPYSn66qF05Sy4t1tvlngHOWo07gU8BHgV+1/jlzWicM8FUI4+blyscVoerUby8fWpIzga8CH66qn7V7WoD5qbeqfglcmGQ9o7tz3zDjkjpJ3gkcq6rvJHnbrOuZwNS/CmHcrPcYVsIt2nN7e3mSNYxC4QtV9bU2PLf1VtXTwB2MdsnXJzn+H9N4LS/W2ebPAp5chvIuBd6V5AfAlxkdTnx6DusEhv8qhFkHw13Atnbmdy2jkzgHZlzTieby9vKMdg2uAx6sqk/Oa71Jzmt7CiR5BaPzIA8yCoirXqbO4/VfBdxe7cB4SFV1TVVtrqqtjH4Pb6+q981bnbBMX4WwXCdLfs1JlB2Mzqg/AvzVjGv5EvAE8Dyj47DdjI4bDwIPA98Azm5rA/x9q/t7wPZlrvWtjI4z7wUOt8eOeasXeCNwd6vzPuCv2/jrgG8zuj3/X4B1bfyM1l9o86+bwe/B2/i/TyXmrs5W0z3tcf/xfzfT/Lv3ykdJnVkfSkiaQwaDpI7BIKljMEjqGAySOgaDpI7BIKljMEjq/C/P9aGT5tzujAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, target = dataset_train[1]\n",
    "\n",
    "img = img.permute(1, 2, 0).numpy()\n",
    "mask = target['masks'][7].numpy()\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "num_classes = 40 # 40 object categories in matterport3d paper https://arxiv.org/pdf/1709.06158.pdf\n",
    "model_weights = \"maskrcnn-weights\"\n",
    "load_weights = True\n",
    "\n",
    "model = build_model(num_classes)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/8]  eta: 0:01:15  lr: 0.000719  loss: 11.2109 (11.2109)  loss_classifier: 3.7597 (3.7597)  loss_box_reg: 0.6675 (0.6675)  loss_mask: 0.8007 (0.8007)  loss_objectness: 5.2780 (5.2780)  loss_rpn_box_reg: 0.7050 (0.7050)  time: 9.4544  data: 0.0737\n",
      "Epoch: [0]  [7/8]  eta: 0:00:07  lr: 0.005000  loss: 4.5586 (6.3439)  loss_classifier: 1.8581 (2.2954)  loss_box_reg: 0.4622 (0.4756)  loss_mask: 0.7215 (0.7362)  loss_objectness: 0.8799 (2.0289)  loss_rpn_box_reg: 0.7050 (0.8078)  time: 7.9081  data: 0.1018\n",
      "Epoch: [0] Total time: 0:01:03 (7.9089 s / it)\n",
      "Epoch: [1]  [0/8]  eta: 0:00:59  lr: 0.005000  loss: 3.0931 (3.0931)  loss_classifier: 0.6225 (0.6225)  loss_box_reg: 0.3848 (0.3848)  loss_mask: 0.6495 (0.6495)  loss_objectness: 0.7298 (0.7298)  loss_rpn_box_reg: 0.7065 (0.7065)  time: 7.4127  data: 0.1106\n",
      "Epoch: [1]  [7/8]  eta: 0:00:07  lr: 0.005000  loss: 2.9757 (2.8913)  loss_classifier: 0.5009 (0.5195)  loss_box_reg: 0.3595 (0.3850)  loss_mask: 0.6317 (0.6323)  loss_objectness: 0.5124 (0.5299)  loss_rpn_box_reg: 0.7277 (0.8245)  time: 7.2064  data: 0.1002\n",
      "Epoch: [1] Total time: 0:00:57 (7.2068 s / it)\n",
      "Epoch: [2]  [0/8]  eta: 0:01:00  lr: 0.005000  loss: 2.5302 (2.5302)  loss_classifier: 0.3520 (0.3520)  loss_box_reg: 0.4336 (0.4336)  loss_mask: 0.5827 (0.5827)  loss_objectness: 0.4572 (0.4572)  loss_rpn_box_reg: 0.7047 (0.7047)  time: 7.5906  data: 0.0965\n",
      "Epoch: [2]  [7/8]  eta: 0:00:07  lr: 0.005000  loss: 2.5175 (2.5258)  loss_classifier: 0.3401 (0.3326)  loss_box_reg: 0.4102 (0.4058)  loss_mask: 0.5775 (0.5770)  loss_objectness: 0.3757 (0.4033)  loss_rpn_box_reg: 0.7309 (0.8071)  time: 7.2031  data: 0.0977\n",
      "Epoch: [2] Total time: 0:00:57 (7.2035 s / it)\n",
      "Epoch: [3]  [0/8]  eta: 0:01:05  lr: 0.000500  loss: 2.3696 (2.3696)  loss_classifier: 0.3421 (0.3421)  loss_box_reg: 0.5141 (0.5141)  loss_mask: 0.5740 (0.5740)  loss_objectness: 0.2845 (0.2845)  loss_rpn_box_reg: 0.6550 (0.6550)  time: 8.1886  data: 0.0974\n",
      "Epoch: [3]  [7/8]  eta: 0:00:07  lr: 0.000500  loss: 2.3696 (2.3667)  loss_classifier: 0.3352 (0.3432)  loss_box_reg: 0.4189 (0.4174)  loss_mask: 0.5592 (0.5578)  loss_objectness: 0.2986 (0.3043)  loss_rpn_box_reg: 0.6550 (0.7439)  time: 7.2801  data: 0.1035\n",
      "Epoch: [3] Total time: 0:00:58 (7.2806 s / it)\n",
      "Epoch: [4]  [0/8]  eta: 0:01:05  lr: 0.000500  loss: 2.3567 (2.3567)  loss_classifier: 0.3321 (0.3321)  loss_box_reg: 0.5024 (0.5024)  loss_mask: 0.5622 (0.5622)  loss_objectness: 0.2746 (0.2746)  loss_rpn_box_reg: 0.6854 (0.6854)  time: 8.2428  data: 0.1086\n",
      "Epoch: [4]  [7/8]  eta: 0:00:07  lr: 0.000500  loss: 2.2183 (2.2749)  loss_classifier: 0.3193 (0.3225)  loss_box_reg: 0.4123 (0.4108)  loss_mask: 0.5473 (0.5419)  loss_objectness: 0.2746 (0.2877)  loss_rpn_box_reg: 0.6854 (0.7120)  time: 7.5279  data: 0.1028\n",
      "Epoch: [4] Total time: 0:01:00 (7.5283 s / it)\n",
      "Epoch: [5]  [0/8]  eta: 0:01:06  lr: 0.000500  loss: 2.3096 (2.3096)  loss_classifier: 0.3195 (0.3195)  loss_box_reg: 0.4701 (0.4701)  loss_mask: 0.5603 (0.5603)  loss_objectness: 0.2857 (0.2857)  loss_rpn_box_reg: 0.6741 (0.6741)  time: 8.3424  data: 0.1073\n",
      "Epoch: [5]  [7/8]  eta: 0:00:07  lr: 0.000500  loss: 2.1972 (2.2142)  loss_classifier: 0.2975 (0.3095)  loss_box_reg: 0.4202 (0.4065)  loss_mask: 0.5256 (0.5319)  loss_objectness: 0.2498 (0.2704)  loss_rpn_box_reg: 0.6741 (0.6959)  time: 7.3840  data: 0.0981\n",
      "Epoch: [5] Total time: 0:00:59 (7.3844 s / it)\n",
      "Epoch: [6]  [0/8]  eta: 0:01:04  lr: 0.000050  loss: 2.2732 (2.2732)  loss_classifier: 0.3266 (0.3266)  loss_box_reg: 0.4933 (0.4933)  loss_mask: 0.5618 (0.5618)  loss_objectness: 0.2569 (0.2569)  loss_rpn_box_reg: 0.6346 (0.6346)  time: 8.1166  data: 0.0973\n",
      "Epoch: [6]  [7/8]  eta: 0:00:07  lr: 0.000050  loss: 2.1845 (2.2038)  loss_classifier: 0.2950 (0.3070)  loss_box_reg: 0.4346 (0.4099)  loss_mask: 0.5203 (0.5302)  loss_objectness: 0.2390 (0.2628)  loss_rpn_box_reg: 0.6346 (0.6939)  time: 7.4786  data: 0.0965\n",
      "Epoch: [6] Total time: 0:00:59 (7.4790 s / it)\n",
      "Epoch: [7]  [0/8]  eta: 0:01:03  lr: 0.000050  loss: 2.3240 (2.3240)  loss_classifier: 0.3168 (0.3168)  loss_box_reg: 0.4826 (0.4826)  loss_mask: 0.5612 (0.5612)  loss_objectness: 0.2509 (0.2509)  loss_rpn_box_reg: 0.7125 (0.7125)  time: 7.9078  data: 0.0932\n"
     ]
    }
   ],
   "source": [
    "# Example of training\n",
    "from examples.instance_segmentation.engine import train_one_epoch\n",
    "import examples.instance_segmentation.utils\n",
    "\n",
    "\n",
    "if load_weights:\n",
    "    # Load model weights\n",
    "    pass\n",
    "\n",
    "# We have to explicitly set the extractor mode because there can only be one instance of an extractor at a time,\n",
    "# so the dataset_train and dataset_test must share the same extractor\n",
    "extractor.set_mode('train')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        # Save model weights\n",
    "        pass\n",
    "\n",
    "# import math\n",
    "\n",
    "# for i, batch in enumerate(data_loader):\n",
    "#     images, targets = batch\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "#     loss_dict = model(images, targets)\n",
    "#     losses = sum(loss for loss in loss_dict.values())\n",
    "#     print(loss_dict)\n",
    "#     print(str(losses.item()) + \"\\n\")\n",
    "    \n",
    "#     if not math.isfinite(losses):\n",
    "#         print(f'not finite loss: {i}')\n",
    "#         break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
